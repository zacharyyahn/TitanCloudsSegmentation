{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73624c9c-918a-43af-ad8a-23fc5aa47b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from torchvision.transforms import v2 as T\n",
    "import utils.extra as extra\n",
    "import cv2\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import dataset\n",
    "from utils.params import Params\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76caedc5-c7ac-49d9-9fa8-028327ddb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the model, load it in, and prepare data transforms\n",
    "\n",
    "model_name = \"maskrcnn_v0\"\n",
    "\n",
    "params = Params(\"saved_models/\" + model_name + \"_hparams.yaml\", \"DEFAULT\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_import = __import__('.'.join(['models', \"maskrcnn\"]),  fromlist=['object'])\n",
    "model = model_import.net(params).to(device)\n",
    "model.load_state_dict(torch.load(\"saved_models/\" + model_name + \".ckpt\"))\n",
    "model.eval()\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "val = model_import.val\n",
    "\n",
    "transforms = [\n",
    "    T.ToDtype(torch.float, scale=True),\n",
    "    T.ToPureTensor(),\n",
    "    T.Resize((512, 512))\n",
    "    #T.RandomHorizontalFlip(0.5)\n",
    "]\n",
    "\n",
    "transformer = T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a1f4f-487b-4f89-ae33-265b2b8bd225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to convert json labels to png\n",
    "\n",
    "def json_to_mask(json_name, height, width):\n",
    "    f = open(\"../Dataset/test/labels/\"+json_name+\".json\")\n",
    "    im = cv2.imread(\"../Dataset/test/images/\"+json_name+\".png\")\n",
    "    annotation = json.load(f)\n",
    "    masks = np.zeros((len(annotation[\"shapes\"]), 512, 512))\n",
    "    if len(annotation[\"shapes\"]) == 0:\n",
    "        masks = np.zeros((1, 512, 512))\n",
    "    for i in range(len(annotation[\"shapes\"])):\n",
    "\n",
    "        #Make sure that the labels are scaled correctly with the images and masks\n",
    "        if im.shape[0] == 512:\n",
    "            points = [[int(point[0]), int(point[1])] for point in annotation[\"shapes\"][i][\"points\"]]\n",
    "        if im.shape[0] == 1024:\n",
    "            points = [[int(point[0] / 2), int(point[1] / 2)] for point in annotation[\"shapes\"][i][\"points\"]]\n",
    "\n",
    "        points = np.array(points)\n",
    "        cv2.fillPoly(masks[i, :, :], pts=[points], color=((i + 1) * 10, 0, 0))\n",
    "        masks[i, :, :] = (masks[i, :, :] == (i + 1) * 10)\n",
    "    return torch.from_numpy(masks).to(dtype=torch.float32)\n",
    "\n",
    "def iou_score(pred, actual, threshold):\n",
    "    actual = 1.0 * actual\n",
    "    pred = 1.0 * (pred >= threshold)\n",
    "    intersection = np.multiply(pred, actual)\n",
    "    union = (np.add(pred, actual) >= 1)\n",
    "    if np.mean(pred) == 0.0 and torch.mean(actual) == 0.0:\n",
    "        return 1.0\n",
    "    return (torch.sum(intersection) / torch.sum(union)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b770ad5-4815-45cb-a16e-f3c33329d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIG ###\n",
    "image_numbers = range(len(os.listdir(\"../Dataset/test/images/\")))\n",
    "save_images = True\n",
    "visualize = False\n",
    "##############\n",
    "\n",
    "image_paths = []\n",
    "for number in image_numbers:\n",
    "    image_paths.append(os.listdir(\"../Dataset/test/images/\")[number])\n",
    "\n",
    "\n",
    "\n",
    "if visualize:  \n",
    "    fig, ax = plt.subplots(len(image_numbers), 3, figsize=(12, 4 * len(image_numbers)))\n",
    "    fig.tight_layout(pad=0.2, w_pad=0.01)\n",
    "    \n",
    "ims = []\n",
    "act_masks = []\n",
    "pred_masks = []\n",
    "ious = []\n",
    "\n",
    "threshold = 0.5\n",
    "score_threshold = 0.5\n",
    "\n",
    "for i in range(len(image_numbers)):\n",
    "    #Load in and process the image for input\n",
    "    im = read_image(\"../Dataset/test/images/\" + image_paths[i])\n",
    "    ims.append(im.squeeze())\n",
    "    im = transformer(im)\n",
    "    im = im.squeeze()\n",
    "    im = torch.stack((im, im, im))\n",
    "\n",
    "    #If there is more than one actual mask, sum them all into one\n",
    "    mask = json_to_mask(image_paths[i][:-4], 512, 512)\n",
    "    act_mask = torch.zeros(mask[0].shape)\n",
    "    for mask_num in range(mask.shape[0]):\n",
    "        act_mask = torch.add(act_mask, mask[mask_num])\n",
    "    act_mask = (act_mask >= 1) #set them all back to 1\n",
    "    act_masks.append(act_mask.squeeze())\n",
    "\n",
    "    #Finally, get the predictions and stack the masks in the same way\n",
    "    out = model([im], None)\n",
    "    print(\"Pred scores:\", out[0][\"scores\"])\n",
    "    wanted_masks = torch.where(out[0][\"scores\"] > score_threshold)\n",
    "    pred_mask = torch.zeros((512, 512))\n",
    "    for index in wanted_masks[0]:\n",
    "        this_mask = out[0][\"masks\"][index].squeeze()\n",
    "\n",
    "        if save_images and np.mean(1.0 * (this_mask:\n",
    "            cv2.imwrite(\"../Dataset/test/preds/\" + image_paths[i][:-11] + \"_\" + str(index.cpu().detach().numpy() + 1) + \".png\", 255.0 * (this_mask > threshold).cpu().detach().numpy())\n",
    "\n",
    "        if this_mask.shape[0] == 3:\n",
    "            this_mask = this_mask[0, :, :].squeeze()\n",
    "        pred_mask = torch.add(pred_mask, this_mask)\n",
    "    pred_mask = (pred_mask >= threshold).detach().cpu().numpy()\n",
    "    pred_masks.append(pred_mask)\n",
    "    \n",
    "    ious.append(iou_score(pred_masks[i], act_masks[i], threshold))\n",
    "\n",
    "if visualize:\n",
    "    if len(image_numbers) == 1:\n",
    "        ax[0].imshow(ims[0], cmap='gray')\n",
    "        ax[0].set_title(\"Image (\" + image_paths[0] + \")\")\n",
    "        ax[1].imshow(act_masks[0])\n",
    "        ax[1].set_title(\"Ground Truth\")\n",
    "        ax[2].imshow(pred_masks[0])\n",
    "        ax[2].set_title(\"Predicted (IoU:\" + str(ious[0]) + \")\")\n",
    "    else:\n",
    "        for i in range(len(image_numbers)):\n",
    "            ax[i, 0].imshow(ims[i], cmap='gray')\n",
    "            ax[i, 0].set_title(\"Image (\" + image_paths[i] + \")\")\n",
    "            ax[i, 1].imshow(act_masks[i])\n",
    "            ax[i, 1].set_title(\"Ground Truth\")\n",
    "            ax[i, 2].imshow(pred_masks[i])\n",
    "            ax[i, 2].set_title(\"Predicted (IoU:\" + str(ious[i]) + \")\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d674bf2-695c-44d4-afb2-3f1d7ae6d0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
